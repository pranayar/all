{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  @VirginAmerica What @dhepburn said.\n",
      "1    @VirginAmerica plus you've added commercials t...\n",
      "2    @VirginAmerica I didn't today... Must mean I n...\n",
      "3    @VirginAmerica it's really aggressive to blast...\n",
      "4    @VirginAmerica and it's a really big bad thing...\n",
      "5    @VirginAmerica seriously would pay $30 a fligh...\n",
      "6    @VirginAmerica yes, nearly every time I fly VX...\n",
      "7    @VirginAmerica Really missed a prime opportuni...\n",
      "8      @virginamerica Well, I didn'tâ€¦but NOW I DO! :-D\n",
      "9    @VirginAmerica it was amazing, and arrived an ...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the social media data set from a CSV file\n",
    "data = pd.read_csv(\"Tweets.csv\")\n",
    "print(data['text'][:10])\n",
    "# Perform text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join([word.lower() for word in word_tokenize(x) if word.lower() not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.000000\n",
      "1    0.000000\n",
      "2   -0.390625\n",
      "3    0.006250\n",
      "4   -0.350000\n",
      "5   -0.516667\n",
      "6    0.466667\n",
      "7    0.200000\n",
      "8    0.000000\n",
      "9    0.466667\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Perform sentiment analysis\n",
    "data['sentiment'] = data['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "print(data['sentiment'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data using TF-IDF\n",
    "# This vectorizes the text data using Term Frequency-Inverse Document Frequency (TF-IDF) vectorization. \n",
    "# It converts the text data into a matrix of TF-IDF features using TfidfVectorizer from scikit-learn.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(data['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This performs topic modeling using Non-negative Matrix Factorization (NMF) from scikit-learn. \n",
    "# It sets the number of topics to 10, initializes the NMF model with 10 components, and fits the \n",
    "# model to the TF-IDF matrix. It then extracts the topic keywords from the model components.\n",
    "num_topics = 10\n",
    "nmf_model = NMF(n_components=num_topics, random_state=42)\n",
    "nmf_model.fit(tfidf)\n",
    "nmf_topic_keywords = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: flight, late, delayed, time, virginamerica, problems, booking, plane, hour, delay\n",
      "Topic 1: jetblue, http, fleek, fleet, rt, love, jfk, guys, stop, good\n",
      "Topic 2: united, dm, bag, http, plane, time, gate, airline, like, yes\n",
      "Topic 3: thank, great, jetblue, ok, response, virginamerica, appreciate, okay, awesome, appreciated\n",
      "Topic 4: thanks, great, jetblue, virginamerica, got, awesome, reply, response, ok, good\n",
      "Topic 5: southwestair, dm, sent, http, follow, hold, help, destinationdragons, love, southwest\n",
      "Topic 6: usairways, hold, hours, help, hour, phone, ve, minutes, plane, time\n",
      "Topic 7: americanair, help, aa, ca, dm, need, phone, hours, guys, dfw\n",
      "Topic 8: service, customer, worst, great, terrible, experience, poor, airline, today, care\n",
      "Topic 9: cancelled, flightled, flights, flighted, tomorrow, hold, help, flight, rebook, today\n"
     ]
    }
   ],
   "source": [
    "keywords = tfidf_vectorizer.get_feature_names_out()\n",
    "topic_keywords = []\n",
    "for topic_weights in nmf_topic_keywords:\n",
    "    top_keywords = [keywords[i] for i in topic_weights.argsort()[:-11:-1]]\n",
    "    topic_keywords.append(', '.join(top_keywords))\n",
    "for i, topic_keywords in enumerate(topic_keywords):\n",
    "    print(f\"Topic {i}: {topic_keywords}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
